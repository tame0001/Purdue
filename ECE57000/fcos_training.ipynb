{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for training each sub data set separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import random\n",
    "import h5py\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/home/tam/') / 'git' / 'cvppp2017_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = h5py.File(path / 'CVPPP2017_training_images.h5')\n",
    "truth = h5py.File(path / 'CVPPP2017_training_truth.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'A1' # select the sub data set A1, A2, A3 or A4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below is to extract the boundary boxes for training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "images = []\n",
    "for plant_id in tqdm(training[dataset].keys()):\n",
    "    boxes = []\n",
    "    for leaf_id in range(1, np.unique(truth[dataset][plant_id]['label'][()]).max()+1):\n",
    "        mask = (truth[dataset][plant_id]['label'][()] == leaf_id).astype(np.uint8)\n",
    "        contours, hierarchy = cv.findContours(mask, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        x, y, w, h = cv.boundingRect(contours[0])\n",
    "        # print(torch.tensor([x, y, x+w, y+h], dtype=torch.float))\n",
    "        boxes.append([x, y, x+w, y+h])\n",
    "\n",
    "    image = Image.fromarray(training[dataset][plant_id]['rgb'][:, :, :3])\n",
    "    images.append(image)\n",
    "\n",
    "    targets.append({\n",
    "        'boxes': torch.tensor(boxes, dtype=torch.float).to(device),\n",
    "        'labels': torch.ones(len(boxes), dtype=torch.int64).to(device)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torchvision.models.detection.FCOS_ResNet50_FPN_Weights.DEFAULT\n",
    "transform = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, targets, transform):\n",
    "        self.images = images\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.images[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return self.transform(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(images, targets, transform)\n",
    "train_loader = torch.utils.data.DataLoader(training_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section before is to draw the ground truth boxex for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_id, plant_id in enumerate(training[dataset].keys()):\n",
    "    box = draw_bounding_boxes(\n",
    "        torchvision.transforms.Compose([torchvision.transforms.PILToTensor()])(images[image_id]), \n",
    "        targets[image_id]['boxes'],\n",
    "        # [weights.meta['categories'][i] for i in result[plant_id][0][\"labels\"]],\n",
    "        colors='red')\n",
    "    to_pil_image(box.detach()).save(path / 'result' / f'{dataset}-{plant_id}-groundtruth.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the normal data loader is not working as FCOS expects a list as input parameters. \n",
    "I created a new data load by slicing a list.\n",
    "Then then feed the training set to re-train the model.\n",
    "At the end, boxes are drawn based on the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 50\n",
    "training_ratio = 0.7\n",
    "order = [*range(len(training_set))]\n",
    "random.shuffle(order)\n",
    "n_training_data = int(len(training_set)*training_ratio)\n",
    "training_order = order[:n_training_data]\n",
    "validate_order = order[n_training_data:]\n",
    "for batch_size in [1, 5, 10]:\n",
    "    torch.cuda.empty_cache()\n",
    "    model = torchvision.models.detection.fcos_resnet50_fpn(\n",
    "        num_classes=2, trainable_backbone_layers=1\n",
    "    ).to(device)\n",
    "    params_to_update = []\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "    optimizer = optim.SGD(params_to_update, lr=1e-3, momentum=0.9)\n",
    "    training_losses = []\n",
    "    evaluation_losses = []\n",
    "    model.train();\n",
    "    for epoch in tqdm(range(max_epoch)):\n",
    "        total_loss = 0\n",
    "        batch_order = training_order.copy()\n",
    "        random.shuffle(batch_order)\n",
    "        while len(batch_order) > 0:\n",
    "            image_batch = []\n",
    "            target_batch = []\n",
    "            if len(batch_order) < batch_size:\n",
    "                current_batch = batch_order.copy()\n",
    "            else:\n",
    "                current_batch = batch_order[:batch_size].copy()\n",
    "\n",
    "            for i in current_batch:\n",
    "                image, target = training_set[i]\n",
    "                image = image.to(device)\n",
    "                image_batch.append(image)\n",
    "                target_batch.append(target)\n",
    "                batch_order.remove(i)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            output = model(image_batch, target_batch)\n",
    "            loss = output['bbox_regression']\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss.item())*len(current_batch)\n",
    "\n",
    "        training_losses.append(total_loss)\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i in validate_order:\n",
    "                image, target = training_set[i]\n",
    "                image = image.to(device)\n",
    "                output = model([image], [target])\n",
    "                loss = output['bbox_regression']\n",
    "                total_loss += float(loss.item())\n",
    "        evaluation_losses.append(total_loss)\n",
    "    \n",
    "    model.eval();\n",
    "    with torch.no_grad():\n",
    "        for image_id, plant_id in enumerate(training[dataset].keys()):\n",
    "            image, target = training_set[image_id]\n",
    "            prediction = model([image.to(device)])\n",
    "            box = draw_bounding_boxes(\n",
    "                torchvision.transforms.Compose([torchvision.transforms.PILToTensor()])(images[image_id]), \n",
    "                prediction[0]['boxes'],\n",
    "                # [weights.meta['categories'][i] for i in result[plant_id][0][\"labels\"]],\n",
    "                colors='red')\n",
    "            to_pil_image(box.detach()).save(path / 'result' / \n",
    "            f'{dataset}-{plant_id}-batch-size={batch_size}-{max_epoch}-new.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(np.log10(training_losses), label='Training')\n",
    "ax.plot(np.log10(evaluation_losses), label='Validation')\n",
    "plt.legend(loc='best');\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ece570')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e07d3286536fa640db5ffb19e9a7b41b55ebd05c0b6a503c91ed85ccfac829dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
