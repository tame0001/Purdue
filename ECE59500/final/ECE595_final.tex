\documentclass[11pt]{article}

% ------
% LAYOUT
% ------
\textwidth 165mm %
\textheight 230mm %
\oddsidemargin 0mm %
\evensidemargin 0mm %
\topmargin -15mm %
\parindent= 10mm

\usepackage[dvips]{graphicx}
\usepackage{multirow,multicol}
\usepackage[table]{xcolor}

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}

\usepackage{caption}
\usepackage{subcaption}

\graphicspath{{./ece595_pics/}} % put all your figures here.
\usepackage[shortlabels]{enumitem}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{float}


\begin{document}
\begin{center}
\Large{\textbf{ECE 595: Final Project}}

Thirawat Bureetes, 10

(Spring 2020)
\end{center}


\subsection*{Exercise 1}

\begin{enumerate}[label=(\alph*)]

%------------------Task a------------------------------

\item 

\begin{enumerate}[label=(\roman*)]

\item \noindent\textbf{Answer}

Subject function $g(\mathbf{x}) = \boldsymbol{\omega^T}\mathbf{x} + \omega_0 = 0$. Try to minimize $l_2$ distance $||\mathbf{x}-\mathbf{x_0}||_2^2$ or $\frac{1}{2}||\mathbf{x}-\mathbf{x_0}||_2^2$.

\begin{align*}
L(\mathbf{x}, \lambda) &= \frac{1}{2}||\mathbf{x}-\mathbf{x_0}||_2^2 + \lambda(\boldsymbol{\omega^T}\mathbf{x} + \omega_0)\\
\nabla_\mathbf{x} L(\mathbf{x}, \lambda) &= \mathbf{x}-\mathbf{x_0} + \lambda\boldsymbol{\omega}\\
0 &= \mathbf{x}-\mathbf{x_0} + \lambda\boldsymbol{\omega} \\
0 &= \boldsymbol{\omega^T}\mathbf{x}-\boldsymbol{\omega^T}\mathbf{x_0} + \lambda||\boldsymbol{\omega}||_2^2 \\
\nabla_\lambda L(\mathbf{x}, \lambda) &= \boldsymbol{\omega^T}\mathbf{x} + \omega_0\\
0 &= \boldsymbol{\omega^T}\mathbf{x} + \omega_0\\
- \omega_0 &= \boldsymbol{\omega^T}\mathbf{x}\\
0 &= - \omega_0-\boldsymbol{\omega^T}\mathbf{x_0} + \lambda||\boldsymbol{\omega}||_2^2 \\
\lambda||\boldsymbol{\omega}||^2 &=  \omega_0 + \boldsymbol{\omega^T}\mathbf{x_0} \\
\lambda^* &=  \frac{\omega_0 + \boldsymbol{\omega^T}\mathbf{x_0}}{||\boldsymbol{\omega}||_2^2} \\
\mathbf{x^*} &= \mathbf{x_0} - \lambda^* \boldsymbol{\omega} \\
\mathbf{x^*} &= \mathbf{x_0} - (\frac{\omega_0 + \boldsymbol{\omega^T}\mathbf{x_0}}{||\boldsymbol{\omega}||_2^2}) \boldsymbol{\omega} \\
\mathbf{x^*} &= \mathbf{x_0} - (\frac{\omega_0 + \boldsymbol{\omega^T}\mathbf{x_0}}{||\boldsymbol{\omega}||_2^2})\frac{ \boldsymbol{\omega}}{||\boldsymbol{\omega}||_2^2} \\
\end{align*}

Subject function $g(\mathbf{x}) = \boldsymbol{\omega^T}\mathbf{x} + \omega_0 = 0$. Try to minimize $l_\infty $ distance $||\mathbf{x}-\mathbf{x_0}||_\infty $.

\begin{align*}
\mathbf{x}-\mathbf{x_0} &= \mathbf{r} \\
\mathbf{x} &= \mathbf{r} + \mathbf{x_0}\\
\boldsymbol{\omega^T}\mathbf{x} + \omega_0 &= 0 \\
\boldsymbol{\omega^T}( \mathbf{r} + \mathbf{x_0}) + \omega_0 &= 0 \\
\boldsymbol{\omega^T} \mathbf{r} &= - \boldsymbol{\omega^T}\mathbf{x_0} - \omega_0  \\
b_0 &= - \boldsymbol{\omega^T}\mathbf{x_0} - \omega_0  \\
\boldsymbol{\omega^T} \mathbf{r} &= b_0  \\
\end{align*}

Now subject function $\boldsymbol{\omega^T} \mathbf{r} = b_0$. Try to minimize $||\mathbf{r}||_\infty $.Utilize Holder’s inequality $|b_0| = |\boldsymbol{\omega^T} \mathbf{r}| \leq ||\boldsymbol{\omega}||_1||\mathbf{r}||_\infty $ or $||\mathbf{r}||_\infty \geq \frac{ |b| }{||\boldsymbol{\omega}||_1}$. This mean $\frac{ |b| }{||\boldsymbol{\omega}||_1}$ is the lowest possible value of $||\mathbf{r}||_\infty$. 

\begin{align*}
||\mathbf{r}||_\infty &= \frac{ |b| }{||\boldsymbol{\omega}||_1} \\
\mathbf{r} &= (\frac{ |b| }{||\boldsymbol{\omega}||_1}) sign(\boldsymbol{\omega})\\
\mathbf{r} &= (\frac{ - \boldsymbol{\omega^T}\mathbf{x_0} - \omega_0  }{||\boldsymbol{\omega}||_1}) sign(\boldsymbol{\omega})\\
\mathbf{x^*} &= \mathbf{x_0} -  (\frac{ \boldsymbol{\omega^T}\mathbf{x_0} + \omega_0  }{||\boldsymbol{\omega}||_1}) sign(\boldsymbol{\omega})
\end{align*}

\item \noindent\textbf{Answer}

Since $g((\mathbf{x})$ is non-linear function, approximate that $g((\mathbf{x}) \approx g(\mathbf{x}^{(k)})+ \nabla_x g(\mathbf{x}^{(k)})^T(\mathbf{x} - \mathbf{x}^{(k)})$. The subject function will become $ g(\mathbf{x}^{(k)})+ \nabla_x g(\mathbf{x}^{(k)})^T(\mathbf{x} - \mathbf{x^{(k)}}) = 0$ Try to minimize $||\mathbf{x}-\mathbf{x_0}||_2^2$

\begin{align*}
 g(\mathbf{x}^{(k)})+ \nabla_x g(\mathbf{x}^{(k)})^T(\mathbf{x} - \mathbf{x}^{(k)}) &= 0 \\
 g(\mathbf{x}^{(k)})+ \nabla_x g(\mathbf{x}^{(k)})^T\mathbf{x} - g(\mathbf{x}^{(k)})^T\mathbf{x}^{(k)} &= 0 \\
\nabla_x g(\mathbf{x}^{(k)})^T\mathbf{x} +  g(\mathbf{x}^{(k)}) -  g(\mathbf{x}^{(k)})^T\mathbf{x}^{(k)} &= 0 \\
\end{align*}

Let $\nabla_x g(\mathbf{x}^{(k)})\mathbf{x} = \boldsymbol{\omega}^{(k)}$ and $  g(\mathbf{x}^{(k)}) -  g(\mathbf{x}^{(k)})^T\mathbf{x}^{(k)} = \omega_0^{(k)}$. Now $\nabla_x g(\mathbf{x}^{(k)})^T\mathbf{x} +  g(\mathbf{x}^{(k)}) -  g(\mathbf{x}^{(k)})^T\mathbf{x}^{(k)} = 0$ can be writen in linear form as $\boldsymbol{\omega^T}\mathbf{x} + \omega_0 = 0$ So the solution will be 


\begin{align*}
\mathbf{x} &= \mathbf{x_0} - (\frac{\omega_0 + \boldsymbol{\omega^T}\mathbf{x_0}}{||\boldsymbol{\omega}||_2^2}) \boldsymbol{\omega} \\
\mathbf{x^{(k+1)}} &= \mathbf{x}^{(k)} - (\frac{\omega_0^{(k)} + \boldsymbol{\omega^{(k)}}^T\mathbf{x}^{(k)}}{||\boldsymbol{\omega}^{(k)}||_2^2}) \boldsymbol{\omega}^{(k)} \\
\boldsymbol{\omega}^{(k)} &= \nabla_x g(\mathbf{x}^{(k)})\mathbf{x} \\
\omega_0^{(k)} &=g(\mathbf{x}^{(k)}) -  g(\mathbf{x}^{(k)})^T\mathbf{x}^{(k)} \\
\mathbf{x}^{(k+1)} & = \mathbf{x}^{(k)} - (\frac{g(\mathbf{x}^{(k)})}{||\nabla_x g(\mathbf{x}^{(k)})||_2^2}) \nabla_x g(\mathbf{x}^{(k)})\\ 
g(\mathbf{x}^{(k)}) &= \frac{1}{2}(\mathbf{x}^{(k)})^T(\mathbf{W_j}-\mathbf{W_t})\mathbf{x}^{(k)} + (\mathbf{w_j}-\mathbf{w_t})^T\mathbf{x}^{(k)} + (w_{j,0} - w_{t,0}) \\
\nabla_x g(\mathbf{x}^{(k)}) &= \frac{1}{2}(\mathbf{W_j}-\mathbf{W_t})\mathbf{x}^{(k)} + (\mathbf{w_j}-\mathbf{w_t}) \\
\end{align*}

\end{enumerate}

%------------------Task b------------------------------

\item  \textbf{Answer}

\begin{enumerate}[label=(\roman*)]

\item \noindent\textbf{Answer}

Subject function $||\mathbf{x}-\mathbf{x_0}||_\infty \leq \eta$. Minimize $ \boldsymbol{\omega^T}\mathbf{x} + \omega_0 $.

\begin{align*}
\mathbf{x}-\mathbf{x_0} &= \mathbf{r} \\
\mathbf{x} &= \mathbf{r} + \mathbf{x_0}\\
\boldsymbol{\omega^T}\mathbf{x} + \omega_0 &= \boldsymbol{\omega^T}( \mathbf{r} + \mathbf{x_0}) + \omega_0\\
\boldsymbol{\omega^T}\mathbf{x} + \omega_0 &= \boldsymbol{\omega^T}\mathbf{r} + \boldsymbol{\omega^T}\mathbf{x_0} + \omega_0\\
\boldsymbol{\omega^T}\mathbf{x} + \omega_0 &= \boldsymbol{\omega^T}\mathbf{r} + b_0\\
\end{align*}

Now subject function becomes $||\mathbf{r}||_\infty \leq \eta $. Minimize $ \boldsymbol{\omega^T}\mathbf{r} + b_0$ Utilize Holder’s inequality

\begin{align*}
- ||\mathbf{r}||_\infty ||\boldsymbol{\omega}||_1  & \leq  \boldsymbol{\omega^T}\mathbf{r} \\
 -\eta  ||\boldsymbol{\omega}||_1& \leq - ||\mathbf{r}||_\infty ||\boldsymbol{\omega}||_1 \\
 -\eta  ||\boldsymbol{\omega}||_1&  \leq  \boldsymbol{\omega^T}\mathbf{r} \\
\end{align*}

$-\eta  ||\boldsymbol{\omega}||_1$ is the lowest possible value of $\boldsymbol{\omega^T}\mathbf{r}$. Consider the minimum value $\mathbf{r} = -\eta (sign(\boldsymbol{\omega}))$ $\mathbf{x^*} = \mathbf{x_0} - \eta (sign(\boldsymbol{\omega}))$

\begin{align*}
\mathbf{x^*} = \mathbf{x_0} - \eta (sign(\boldsymbol{\omega})
\end{align*}

\item \noindent\textbf{Answer}

Define lose function $J(\mathbf{x}) = -g(\mathbf{x})$. Try to maximize $J(\mathbf{x})$ subject to $||\mathbf{x}-\mathbf{x_0}||_\infty \leq \eta$. Define $\mathbf{x} = \mathbf{r} + \mathbf{x_0}$, then $J(\mathbf{x}) = J(\mathbf{r} + \mathbf{x_0}) \approx J(  \mathbf{x_0}) + \nabla_x J(  \mathbf{x_0})^T \mathbf{r}$. 

Rewrite optimize objective function to maximize $J(\mathbf{x_0}) + \nabla_x J(  \mathbf{x_0})^T \mathbf{r}$ or  minimize $-J(\mathbf{x_0}) - \nabla_x J(  \mathbf{x_0})^T \mathbf{r}$ subject to  $||\mathbf{r}||_\infty \leq \eta $ Define $-J(\mathbf{x_0}) = b_0$ and $-\nabla_x J(  \mathbf{x_0}) = \boldsymbol{\omega}$, the objective function can be reform to $ \boldsymbol{\omega^T}\mathbf{r} + b_0$ which similar to previous problem. 

\begin{align*}
\mathbf{x^*} &= \mathbf{x_0} - \eta (sign(\boldsymbol{\omega}) \\
\mathbf{x^*} &= \mathbf{x_0} - \eta (sign({-\nabla_x J(  \mathbf{x_0})}) \\
\mathbf{x^*} &= \mathbf{x_0} + \eta (sign({\nabla_x J(  \mathbf{x_0})}) \\
\mathbf{x^*} &= \mathbf{x_0} + \eta (sign({\nabla_x (-g(  \mathbf{x_0})})) \\
\mathbf{x^*} &= \mathbf{x_0} - \eta (sign({\nabla_x (g( \mathbf{x_0})})) \\
\nabla_x g(\mathbf{x}_0) &= \frac{1}{2}(\mathbf{W_j}-\mathbf{W_t})\mathbf{x}_0 + (\mathbf{w_j}-\mathbf{w_t}) \\
\end{align*}

\item \noindent\textbf{Answer}

\begin{align*}
J(\mathbf{x}) &\approx J(  \mathbf{x_0}) + \nabla_x J(  \mathbf{x_0})^T \mathbf{r} \\
&= J(  \mathbf{x_0}) + \nabla_x J(  \mathbf{x_0})^T (\mathbf{x} - \mathbf{x_0}) \\
&= J(  \mathbf{x_0}) + \nabla_x J(  \mathbf{x_0})^T (\mathbf{x}) - \nabla_x J(  \mathbf{x_0})^T(\mathbf{x_0}) \\
\end{align*}

The objective is to maximize $J(\mathbf{x})$ or $J(  \mathbf{x_0}) + \nabla_x J(  \mathbf{x_0})^T (\mathbf{x}) - \nabla_x J(  \mathbf{x_0})^T(\mathbf{x_0})$. As only $\nabla_x J(  \mathbf{x_0})^T (\mathbf{x})$ has  $\mathbf{x}$ term so the objective function can be reduce to maximize $\nabla_x J(  \mathbf{x_0})^T (\mathbf{x})$ subject to  $||\mathbf{x}-\mathbf{x_0}||_\infty \leq \eta$ and $0 \leq \mathbf{x} \leq 1$

Rewrite into iteration format, objective function is $\nabla_x J(  \mathbf{x}^{(k)})^T (\mathbf{x})$  subject to  $||\mathbf{x}-\mathbf{x}^{(k)}||_\infty \leq \eta$ and $0 \leq \mathbf{x} \leq 1$. Consider without subject function  $0 \leq \mathbf{x} \leq 1$, the optimization is identical to previous problem. 

\begin{align*}
\mathbf{x^*} &= \mathbf{x_0} + \eta (sign({\nabla_x J(  \mathbf{x_0})}) \\
\mathbf{x}^{(k+1)} &= \mathbf{x}^{(k)} + \eta (sign({\nabla_x J(  \mathbf{x}^{(k)})}) \\
\end{align*}

Adding the conidtion that $0 \leq \mathbf{x} \leq 1$

\begin{align*}
\mathbf{x}^{(k+1)} &= P_{[0,1]}\{\mathbf{x}^{(k)} + \eta (sign({\nabla_x J(  \mathbf{x}^{(k)})})\} \\
\mathbf{x}^{(k+1)} &= P_{[0,1]}\{\mathbf{x}^{(k)} - \eta (sign({\nabla_x g(  \mathbf{x}^{(k)})})\} \\
\nabla_x g(\mathbf{x}^{(k)}) &= \frac{1}{2}(\mathbf{W_j}-\mathbf{W_t})\mathbf{x}^{(k)} + (\mathbf{w_j}-\mathbf{w_t}) \\
\end{align*}

\end{enumerate}

%------------------Task c------------------------------

\item

\begin{enumerate}[label=(\roman*)]

\item \noindent\textbf{Answer}

As this is a binary class problem, the objective function can be writen as $||\mathbf{x}-\mathbf{x_0}||_2^2+\lambda(\boldsymbol{\omega^T}\mathbf{x} + \omega_0)$ or $\frac{1}{2}||\mathbf{x}-\mathbf{x_0}||_2^2+\lambda(\boldsymbol{\omega^T}\mathbf{x} + \omega_0)$

\begin{align*}
\nabla_x(\frac{1}{2}||\mathbf{x}-\mathbf{x_0}||_2^2+\lambda(\boldsymbol{\omega^T}\mathbf{x} + \omega_0)) &= 0\\
(\mathbf{x^*}-\mathbf{x_0}) + \lambda\boldsymbol{\omega} &= 0 \\
\mathbf{x^*} &= \mathbf{x_0} - \lambda\boldsymbol{\omega}
\end{align*}

\item \noindent\textbf{Answer}

Minimize $||\mathbf{x}-\mathbf{x_0}||_2^2 + \lambda \cdot max(max\{g_j(\mathbf{x})\}-g_t(\mathbf{x}),0)$. Define $i^*(\mathbf{x})$ as the index if maximum response from $g_j(\mathbf{x})$ for $j \neq t$. 

\begin{align*}
\nabla_x max(g_{i^*}(\mathbf{x})-g_t(\mathbf{x}),0) &= \nabla_x g_{i^*}(\mathbf{x}) - \nabla_xg_t(\mathbf{x}) &&  if \ g_{i^*}(\mathbf{x})-g_t(\mathbf{x}) > 0\\
\nabla_x max(g_{i^*}(\mathbf{x})-g_t(\mathbf{x}),0) &=\mathbb{I} \{g_{i^*}(\mathbf{x})-g_t(\mathbf{x})>0\} \cdot (\nabla_x g_{i^*}(\mathbf{x}) - \nabla_xg_j(\mathbf{x})) \\
\end{align*}
So the grandient of the objective function is  

\begin{align*}
\nabla_x \varphi(\mathbf{x}; i^*) &= \nabla_x( ||\mathbf{x}-\mathbf{x_0}||_2^2 + \lambda \cdot max(g_{i^*}(\mathbf{x})-g_t(\mathbf{x}),0)) \\
\nabla_x \varphi(\mathbf{x}; i^*) &= 2(\mathbf{x}-\mathbf{x_0})+ \lambda \cdot \mathbb{I} \{g_{i^*}(\mathbf{x})-g_t(\mathbf{x})>0\} \cdot (\nabla_x g_{i^*}(\mathbf{x}) - \nabla_xg_j(\mathbf{x})) \\
i^* &= argmax\{ g_j(x^{(k)})\} \\
\mathbf{x}^{(k+1)} &= \mathbf{x}^{(k)} - \alpha \nabla_x \varphi(\mathbf{x}^{(k)}; i^*)
\end{align*}

\end{enumerate}

\end{enumerate}

%-------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection*{Exercise 2}

\noindent\textbf{Answer}

\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_0_1}
  \caption{Original image}
  \label{fig:original_image}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_0_3}
  \caption{Classifier result}
  \label{fig:original_classify_result}
\end{subfigure}

\caption{Original Cat image}
\label{fig:}
\end{figure}

\begin{figure}[H]
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_2_1}
  \caption{Perturbed image}
  \label{fig:}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_2_2}
  \caption{Perturbation added}
  \label{fig:}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_2_3}
  \caption{Classifier result}
  \label{fig:}
\end{subfigure}

\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_2_4}
  \caption{CW at $50^{th}$ iteration}
  \label{fig:}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_2_5}
  \caption{CW at $10^{th}$ iteration}
  \label{fig:}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_2_6}
  \caption{CW at $5^{th}$ iteration}
  \label{fig:}
\end{subfigure}

\caption{CW Attack - Non-Overlapping Patches $\lambda$ = 1}
\label{fig:}
\end{figure}

\begin{figure}[H]
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_1_1}
  \caption{Perturbed image}
  \label{fig:}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_1_2}
  \caption{Perturbation added}
  \label{fig:}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_1_3}
  \caption{Classifier result}
  \label{fig:}
\end{subfigure}

\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_1_4}
  \caption{CW at $50^{th}$ iteration}
  \label{fig:}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_1_5}
  \caption{CW at $10^{th}$ iteration}
  \label{fig:}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_1_6}
  \caption{CW at $5^{th}$ iteration}
  \label{fig:}
\end{subfigure}

\caption{CW Attack - Non-Overlapping Patches $\lambda$ = 5}
\label{fig:}
\end{figure}

\begin{figure}[H]
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_3_1}
  \caption{Perturbed image}
  \label{fig:}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_3_2}
  \caption{Perturbation added}
  \label{fig:}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_3_3}
  \caption{Classifier result}
  \label{fig:}
\end{subfigure}

\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_3_4}
  \caption{CW at $50^{th}$ iteration}
  \label{fig:}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_3_5}
  \caption{CW at $10^{th}$ iteration}
  \label{fig:}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_3_6}
  \caption{CW at $5^{th}$ iteration}
  \label{fig:}
\end{subfigure}

\caption{CW Attack - Non-Overlapping Patches $\lambda$ = 10}
\label{fig:}
\end{figure}

Figure \ref{fig:original_image} is an original cat image. And figure \ref{fig:original_classify_result} is the clasification result.
The clasifier aim to clasify each pixel into 2 classes: grass represent with black and cat represent in white. 
The CW attact is used to fool the classifier on cat pixel. The goal is to make classifier unable to detect the cat pixel. Or in the other hand, the classicifation result will be all black. 

Figure 2 - 4 are the result of CW attack with $\lambda$ = 1, 5, and 10 repectively with the Frobenius norm 41.89, 89.15 and 99.13 respectively. For each $\lambda$ value, the final perturbed images are shown in subfigure (a). The limit of iteration is set to 300. Subfigure (b) shows the purturbation and (c) shows the classification result. Subfigure (d), (e) and (f) shows the classification result of purturbe image after 50, 10 and 5 iterations repectively.

Number of iteration directly relates to the attack result. The more of iteration, the more purturb on the image which makes the classifier fails to detect the cat images. This support by the number of white pixel disappear when the number of iteration is high. The value of $\lambda$ effect the number of iteration until reaching the final result. With $\lambda$ = 1, there are still plently of white pixel after 300 iteration. While at $\lambda$ = 10, most of the white pixel is disappear after 50 iteration. 

\begin{figure}[H]
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_4_1}
  \caption{$\alpha$ = 0.0001}
  \label{fig:}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_4_2}
  \caption{$\alpha$ = 0.001}
  \label{fig:}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{final_2_4_3}
  \caption{$\alpha$ = 0.01}
  \label{fig:}
\end{subfigure}

\caption{CW Attack with different $\alpha$ ($\lambda$ = 1, limit at 100 iterations)}
\label{fig:}
\end{figure}

Figure 5 shows the result of attacking at different $\alpha$ value. It is clear that with higher $\alpha$ value, the more white pixel disappear. With $\alpha$ = 0.01, all white pixel is disspear after 100 iteration and the value of $\lambda$ is set at 1. The perturbation norm is 99.13. If the $alpha$ value is increased to 0.1, the result doesn't change and the perturbation norm is also 99.13. This mean that the output of attacking is optimized.




%-------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection*{Exercise 3}




\end{document}

